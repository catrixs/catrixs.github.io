---
layout: post
title: Factorization Machines
---

在点击预估、兴趣学习等实际应用场景中，逻辑回归模型模型本身不能去描述特征之间的交互因素，比如用户与物料物料特征组合，用户与上下文特征组合，推荐物料与上下文的特征组合，在线性模型中无法学习到。依赖于对数据和业务的深入理解，进行特征的选取和组合方案，问题是过于依赖算法工程师的能力，而且高维特征的稀疏性更加严重，导致线性回归无法拟合。

FM则把特征组合纳入到模型中，通过矩阵分解技术解决高维特征组合稀疏性问题。通过一个模型，直接训练用户基本信息特征，物料基本特征，打分特征，时间特征，物料相关性等特征，如下图：

> 2010年，日本大阪大学(Osaka University)的 Steffen Rendle 在矩阵分解(MF)、SVD++、PITF、FPMC 等基础之上，归纳出针对高维稀疏数据的因子机(Factorization Machine, FM)模型。因子机模型可以将上述模型全部纳入一个统一的框架进行分析。并且，Steffen Rendle 实现了一个单机多线程版本的 libFM。在随后的 KDD Cup 2012，track2 广告点击率预估(pCTR)中，国立台湾大学和 Opera Solutions[5] 两只队伍都采用了 FM，并获得大赛的冠亚军而使得 FM 名声大噪。随后，台湾大学的 Yuchin Juan 等人在总结自己在两次比赛中的经验以及 Opera Solutions 队伍中使用的 FM 模型的总结，提出了一般化的 FFM 模型，并实现了单机多线程版的 libFFM，并做了深入的试验研究。事实上，Opera Solutions 在比赛中用的 FM 就是FFM。

![](/images/fm.jpg)

# 模型定义

### 数学部分（看不懂可以直接跳过）

以二维空间为例，FM把$`x_{i}`$和$`x_{j}`$特征的关系纳入模型，如下：

```math
y(x) = w0 + \sum_{i=1}^{n} w_{i}x_{i} + \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{ij}x_{i}x_{j}
```

但是模型对于稀疏数据并不适用，原因在于由于数据的稀疏性，$`x_{i}x_{j}`$的乘积巨大部分会为0，换句说就是两个特征在样本数据中并没有出现过交互的情况，导致交叉特征分量$`w{ij}`$大部分为0。对于稀疏矩阵，可以通过因式分解来估计缺失部分的数据。同MF模型相似，把$`w_{ij}`$被改写为：$`w_{ij} = V_{i}^{T}V_{j} = \sum_{f=1}^{k} v_{if}v_{jf}`$。如果K足够大时，对于任意对称正定实矩阵$`W\in R^{n*n}`$，均存在实矩阵$`V \in R^{n*k}`$，使得$`W=VV^{T}`$成立。

于是FM模型可以表示为：

```math
y(x) = w0 + \sum_{i=1}^{n} w_{i}x_{i} + \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} V_{i}^{T} V_{j} x_{i}x_{j}
```

从上面的公式可以看出，线性部分的复杂度是$`O(n)`$，因式分解部分复杂度是$`O(kn^{2})`$，但是通过变换可以把因式分解部分复杂度降低到$`O(kn)`$：

```math
\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} (V_{i}^{T}V_{j})x_{i}x_{j}
```

```math
= 1/2(\sum_{i=1}^{n} \sum_{j=1}^{n} (V_{i}^{T}V_{j})x_{i}x_{j} - \sum_{i=1}^{n} (V_{i}^{T}V_{i})x_{i}x_{i})
```

```math
= 1/2(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} (v_{if}v_{jf})x_{i}x_{j} - \sum_{i=1}^{n} \sum_{f=1}^{k} (v_{if}^{2})x_{i}^{2})
```

```math
= 1/2\sum_{f=1}^{k} (\sum_{i=1}^{n} (v_{if}x_{i} \sum_{j=1}^{n} (v_{jf}x_{j}) - \sum_{i=1}^{n} (v_{if}^{2})x_{i}^{2})
```

```math
= 1/2\sum_{f=1}^{k} [ (\sum_{i=1}^{n} (v_{if}x_{i})^{2} - \sum_{i=1}^{n} (v_{if}^{2})x_{i}^{2})]
```

于是FM模型最终可以表达成，实际复杂度就变成了$`O(kN_{z}(x))`$

```math
y(x) = w0 + \sum_{i=1}^{n} w_{i}x_{i} + 1/2\sum_{f=1}^{k} [ (\sum_{i=1}^{n} (v_{if}x_{i})^{2} - \sum_{i=1}^{n} (v_{if}^{2})x_{i}^{2})]
```

### 实现部分

下面主要实现代码是基于[TensorFlow实现] (https://github.com/geffy/tffm)，通过TensorFlow定义FM模型大体上可以分为

- 初始化权重参数
- 初始化输入占位符
- 构建计算图

#### 初始化权重参数（tf.Variable）

```python
    def init_learnable_params(self):
        self.w = [None] * self.order      ## 这里w[0]是线性部分权重，w[i]是m * k维的特征权重
        for i in range(1, self.order + 1):
            r = self.rank
            if i == 1:
                r = 1
            rnd_weights = tf.random_uniform([self.n_features, r], -self.init_std, self.init_std)
            self.w[i - 1] = tf.verify_tensor_all_finite(
                tf.Variable(rnd_weights, trainable=True, name='embedding_' + str(i)),
                msg='NaN or Inf in w[{}].'.format(i-1))
        self.b = tf.Variable(self.init_std, trainable=True, name='bias')
        tf.summary.scalar('bias', self.b)        #可以在TensorBoard中查看bias的变化趋势
```

#### 初始输入参数（tf.placeholder）

```python
    def init_placeholders(self):
        if self.input_type == 'dense':
            self.train_x = tf.placeholder(tf.float32, shape=[None, self.n_features], name='x')
        else:
            with tf.name_scope('sparse_placeholders') as scope:
                self.raw_indices = tf.placeholder(tf.int64, shape=[None, 2], name='raw_indices')
                self.raw_values = tf.placeholder(tf.float32, shape=[None], name='raw_data')
                self.raw_shape = tf.placeholder(tf.int64, shape=[2], name='raw_shape')
            # tf.sparse_reorder is not needed since scipy return COO in canonical order
            self.train_x = tf.SparseTensor(self.raw_indices, self.raw_values, self.raw_shape)
        self.train_y = tf.placeholder(tf.float32, shape=[None], name='Y')
```

![fm_tensorflow_3](/images/fm_tensorflow_3.png)
![fm_tensorflow_4](/images/fm_tensorflow_4.png)

#### 构建计算图（实际算法部分）

$`\sum_{i=1}^{n} (v_{if}x_{i})^{2}`$这部分实际就是$`(V * X)^{2}`$，可以直接通过TensorFlow操作定义：

```python
raw_dot = utils.matmul_wrapper(self.train_x, self.w[i - 1], self.input_type)
dot = tf.pow(raw_dot, i) 
```

另外对于模型的后半部分：$`- \sum_{i=1}^{n} (v_{if}^{2})x_{i}^{2}`$，大部分是计算是可以复用的，可以把$`v_{if}^{2}`$和$`x_{i}^{2}`$缓存：

```python

    def pow_matmul(self, order, pow):
        if pow not in self.x_pow_cache:
            x_pow = utils.pow_wrapper(self.train_x, pow, self.input_type)
            self.x_pow_cache[pow] = x_pow
        if order not in self.matmul_cache:
            self.matmul_cache[order] = {}
        if pow not in self.matmul_cache[order]:
            w_pow = tf.pow(self.w[order - 1], pow)
            dot = utils.matmul_wrapper(self.x_pow_cache[pow], w_pow, self.input_type)
            self.matmul_cache[order][pow] = dot
        return self.matmul_cache[order][pow]
```

```python
def init_main_block(self):
        self.x_pow_cache = {}
        self.matmul_cache = {}
        # 这里outputs表示计算结果，是n*1维tensor
        self.outputs = self.b

        # 这里name_scope可以在TensorBoard中把这部分计算作为一个整体
        with tf.name_scope('linear_part') as scope:      
            # 这里实际上是tf.matmul(A, B)，对应公式中线性部分
            contribution = utils.matmul_wrapper(self.train_x, self.w[0], self.input_type) 
        self.outputs += contribution
        for i in range(2, self.order + 1):
            with tf.name_scope('order_{}'.format(i)) as scope:
                # 这里dot就是公式中(v(if)x(i))^2
                raw_dot = utils.matmul_wrapper(self.train_x, self.w[i - 1], self.input_type)
                dot = tf.pow(raw_dot, i)      
                if self.use_diag:
                    contribution = tf.reshape(tf.reduce_sum(dot, [1]), [-1, 1])
                    contribution /= 2.0**(i-1)
                else:
                    initialization_shape = tf.shape(dot)
                    for in_pows, out_pows, coef in utils.powers_and_coefs(i):
                        product_of_pows = tf.ones(initialization_shape)
                        for pow_idx in range(len(in_pows)):
                            pmm = self.pow_matmul(i, in_pows[pow_idx])
                            product_of_pows *= tf.pow(pmm, out_pows[pow_idx])
                        # 这里对应公式中的
                        dot -= coef * product_of_pows
                    contribution = tf.reshape(tf.reduce_sum(dot, [1]), [-1, 1])
                    contribution /= float(math.factorial(i)) # 这里表示1/2
            self.outputs += contribution
```

# 损失函数

利用FM模型可以回归、分类、排序，不同任务对应的损失函数不同：

- 回归：损失函数是MLSE(minimal least square error)
- 分类：损失函数通常使用最大释然估计，$`L = \sum_{i=1}^{N} -ln sigmod(y'y)`$
- 排序：[Pointwise, Pairwise, Listwise](http://blog.csdn.net/hguisu/article/details/7989489)

### 实现部分

```python
def loss_logistic(outputs, y):
    margins = -y * tf.transpose(outputs)
    raw_loss = tf.log(tf.add(1.0, tf.exp(margins)))
    # y, idx = tf.unique()
    return tf.minimum(raw_loss, 100, name='truncated_log_loss')

def loss_mse(outputs, y):
    return tf.pow(y -  tf.transpose(outputs), 2, name='mse_loss')
```

# 学习

以梯度下降学习算法为例。梯度下降算法是通过沿着目标函数J(θ)参数θ的梯度（一阶导数）相反方向−∇θJ(θ)来不断更新模型参数来到达目标函数的极小值点（收敛），更新步长为η。有三种梯度下降算法框架，它们不同之处在于每次学习（更新模型参数）使用的样本个数，每次更新使用不同的样本会导致每次学习的准确性和学习时间不同。

### 批量梯度下降（BGD）

每次使用全量的训练集样本来更新模型参数，批量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新

```math
w := w - \lambda / n \sum_{i=1}^{n} \Delta Loss(y', y)
```

### 随机梯度下降（SGD）

随机梯度下降算法每次从训练集中随机选择一个样本来进行学习，每次的学习是非常快速的，并且可以进行在线更新。随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)。

```math
w := w - \lambda / n \sum_{i=1}^{n} \Delta Loss(y_{i}', y_{i})
```

### 小批量梯度下降（MiniBGD）

Mini-batch 梯度下降综合了 batch 梯度下降与 stochastic 梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择 m,m<n 个样本进行学习

### FM梯度函数

因为FM的模型计算复杂度是线性的，因此模型的梯度为：

```math
\frac{\delta y(x)}{\delta \theta } = \left\{\begin{matrix} 1, \theta = w0;
\\ x_{i}, \theta = w_{i}, i = 1, 2, ..., n;
\\ x_{i} \sum_{j=1 j j!=i}^{n} v_{jf} xj, \theta = vif, i = 1, 2, ..., n;

\end{matrix}\right.
```

### 实现部分

如果没有TensorFlow框架，就需要自己来实现模型的梯度下降学习算法，[libfm](https://github.com/srendle/libfm)中关于sgd的核心C++代码：

```C++
void fm_SGD(fm_model* fm, const double& learn_rate, sparse_row<DATA_FLOAT> &x, const double multiplier, DVector<double> &sum) {
	if (fm->k0) {
		double& w0 = fm->w0;
		w0 -= learn_rate * (multiplier + fm->reg0 * w0);
	}
	if (fm->k1) {
		for (uint i = 0; i < x.size; i++) {
			double& w = fm->w(x.data[i].id);
			w -= learn_rate * (multiplier * x.data[i].value + fm->regw * w);
		}
	}	
	for (int f = 0; f < fm->num_factor; f++) {
		for (uint i = 0; i < x.size; i++) {
			double& v = fm->v(f,x.data[i].id);
			double grad = sum(f) * x.data[i].value - v * x.data[i].value * x.data[i].value; 
			v -= learn_rate * (multiplier * grad + fm->regv * v);
		}
	}	
}
```

TensorFlow已经把梯度下降等算法内置到框架中，只要定义好模型的Variable和Loss，再选择适合的Optimizer就可以完成学习过程。首先需要定义模型的输出值，下面的target就是模型的输出，

```python
    def init_target(self):
        with tf.name_scope('target') as scope:
            self.target = self.reduced_loss + self.reg * self.regularization
            self.checked_target = tf.verify_tensor_all_finite(
                self.target,
                msg='NaN or Inf in target value', 
                name='target')
            tf.summary.scalar('target', self.checked_target)
```

有了目标值之后，就可以使用tf.train.GradientDescentOptimizer来实现学习过程，learning_rate决定了每次梯度下降的幅度，越小的值越有可能接近最优解，同时也需要更多次的迭代，耗时也较长。

```python
            self.trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(self.checked_target)
```

![fm_tensorflow_1](/images/fm_tensorflow_1.png)
![fm_tensorflow_2](/images/fm_tensorflow_2.png)

# How To Debug

例如在模型训练时，遇到如下异常，应该如何定位问题呢？

```python
Caused by op u'learnable_params/VerifyFinite/CheckNumerics', defined at:
  File "feed_recsys.py", line 76, in <module>
    model.fit('/Users/chenfei/weibo/mapreduce/features.txt')
    ......
InvalidArgumentError (see above for traceback): NaN or Inf in w[0]. : Tensor had NaN values
      [[Node: learnable_params/VerifyFinite/CheckNumerics = CheckNumerics[T=DT_FLOAT, _class=["loc:@learnable_params/embedding_1"], message="NaN or Inf in w[0].", _device="/job:localhost/replica:0/task:0/cpu:0"](learnable_params/embedding_1/read)]]
```

因为TensorFlow在Session运行之前，无法获得Variable具体值。TensorFlow提供了一个tf.Print操作符，以便于在TensorFlow Session运行期间动态观察某个Tensor的值的变化。使用方式可以把报错的embedding部分的weigth tensor通过tf.Print操作打印出来。这里注意一下，tf.Print并不是立即打印出Tensor的值，而是在Tensor的运行期，当计算图走到tf.Print算子的时候进行打印，所以代码中应该是使用tf.Print返回的Tensor，而不应该使用原来的Tensor，否则是无任何输出的。

```python
            w = tf.Variable(rnd_weights, trainable=True, name='embedding_' + str(i - 1))
            w = tf.Print(w, [tf.reduce_any(tf.is_(w))], "embedding_" + str(i) + " : ")
            self.w[i - 1] = tf.verify_tensor_all_finite(w, msg='NaN or Inf in w[{}].'.format(i-1))
```

运行后发现一个规律，当reduced_loss不降反升时，就出现了```Tensor had NaN values```异常。与[AdamOptimizer](https://github.com/tensorflow/tensorflow/issues/323)，和[log(0)](http://quabr.com/33712178/tensorflow-nan-bug)有关。

```
I tensorflow/core/kernels/logging_ops.cc:79] reduced_loss: [0.522131]
I tensorflow/core/kernels/logging_ops.cc:79] embedding_1 : [0]
I tensorflow/core/kernels/logging_ops.cc:79] embedding_2 : [0]
I tensorflow/core/kernels/logging_ops.cc:79] raw_loss: [0]
I tensorflow/core/kernels/logging_ops.cc:79] reduced_loss: [0.7341007]
I tensorflow/core/kernels/logging_ops.cc:79] embedding_1 : [1]
I tensorflow/core/kernels/logging_ops.cc:79] embedding_2 : [1]
```

# 参考

- [Factorization Machines with libFM](http://www.csie.ntu.edu.tw/~b97053/paper/Factorization%20Machines%20with%20libFM.pdf)
- [因子机深入解析](https://tracholar.github.io/machine-learning/2017/03/10/factorization-machine.html)
- [Factorization Machines 学习笔记（一）预测任务](http://blog.csdn.net/itplus/article/details/40534885)
- [SDG](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
- [Tensorflow crashed when using AdamOptimizer](https://github.com/tensorflow/tensorflow/issues/323)
- [Tensorflow NaN bug](http://quabr.com/33712178/tensorflow-nan-bug)